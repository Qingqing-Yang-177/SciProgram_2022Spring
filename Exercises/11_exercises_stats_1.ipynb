{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises, stats 1\n",
    "\n",
    "\n",
    "This notebook provides practice problems focused on dataframe manipulation, plotting, and running correlations, t-tests and regression.\n",
    "\n",
    "These exercises will have you load data from three different csv files:\n",
    "\n",
    "bounce.csv\n",
    "\n",
    "demo_and_data.csv\n",
    "\n",
    "nyc_covid.csv\n",
    "\n",
    "These are on Brightspace under Content / Data /\n",
    "\n",
    "You will want to download that and put the files somewhere so that you can load them using pd.read_csv() as we've done on previous assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "You will need to import the following libraries:\n",
    "\n",
    "- scipy.stats\n",
    "- numpy\n",
    "- seaborn\n",
    "- pandas\n",
    "- pingouin\n",
    "\n",
    "Examples of doing this can be found at the top of the various viz_and_stats notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Paired t-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem we will use a sample dataset stored in the file `demo_and_data.csv`.\n",
    "\n",
    "The file contains data from a set of participants who participated in a study with two trial types: congruent and incongruent. Each row of the dataframe has data for one participant in either the congruent or incongruent trial_type condition. Each person completed both conditions so we have a within-person design.\n",
    "\n",
    "In addition to the `trial_type` column, the data includes the average percent correct for those trials (`acc`), the average response time (`RT`), the participant id (`uid`) and some demographic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pandas and load the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the possible unique values of the trial_type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In preparation for running a t-test comparing RT values for each of the two trial_types in the dataframe we need to pull the RT data for each trial_type into their own variable.\n",
    "\n",
    "Use grouby() and get_group() to pull the data for each trial type into their own dataframes.\n",
    "\n",
    "If you loaded the data into a dataframe called df, then this code would give you a smaller dataframe for only those places where trial_type was 'congruent':\n",
    "\n",
    "```python\n",
    "cong_df = df.groupby('trial_type').get_group('congruent').copy()\n",
    "```\n",
    "\n",
    "#### Make two new dataframes, one that has the data for 'congruent' trial type and the other for 'incongruent' trial type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the data\n",
    "\n",
    "We have RT data for each participant in the two trial types. To compare these RT values between conditions we can do a t-test. \n",
    "\n",
    "Our data are paired: we have both measurements for each person, so we will want to run a paired t-test. \n",
    "\n",
    "The paired t-test assumes that the numbers that we give it are lined up within pair, so we need to make sure that the two dataframes we created in P1-B are in the same `uid` order.\n",
    "\n",
    "Use the pandas dataframe sort_values() function. This function will take in a column name to sort by, and you should include an inplace=True argument to preserve the changes being made to the dataframe.\n",
    "\n",
    "\n",
    "This code will sort the rows in a dataframe called df based on the values in 'column_name' and the sorted dataframe will be the new version (due to inplace=True)\n",
    "\n",
    "```python\n",
    "df.sort_values('column_name',inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pinguoin (imported as pg) to run a paired t-test comparing the 'RT' in the congruent to the incongruent trials.\n",
    "\n",
    "the syntax is like this:\n",
    "\n",
    "```python\n",
    "pg.ttest(x=some_data, y=some_other_data, paired=True)\n",
    "```\n",
    "\n",
    "\n",
    "The x= and y= arguments are the sets of paired measurements we are comparing. Previously you pulled the congruent and incongruent into different data frames. So you can point x and y to the 'RT' column in those two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your t-test code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The dataset includes children and we'd like to know if they could perform the task better than random. The `acc` column gives the average percent correct for each person, and .5 means they were not performing better than random chance.\n",
    "    \n",
    "Run a one-sample t-test comparing the `acc` for children in the dataset against a comparison value of .5\n",
    "    \n",
    "You will need to find a way to get just the parts of the dataframe where age_group is 'child'. In your answer you should report whether the childrens accuracy was significantly different from .5\n",
    "\n",
    "\n",
    "Pingouin one-sample t-test:\n",
    "\n",
    "```python\n",
    "pg.ttest(x=some_numbers, y=population_mean)\n",
    "```\n",
    "\n",
    "x is an array of numbers and the one-sample t-test assess whether they were likely to be random samples from a distribution with a mean value as specified in the y= input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid dataset\n",
    "\n",
    "The _nyc_covid.csv_ file has data for the following variables:\n",
    "\n",
    "\n",
    "- zip: the zip code where the data were collected\n",
    "- borough: the borough where the zip code is located\t\n",
    "- neighborhood: the neighborhood where the zip code is located\t\t\n",
    "- deathper100k: covid deaths per 100k people in this zip code\t\n",
    "- whitepct: percent of population identified as white in this zip code\n",
    "- medianincome: median income level across people in this zip code\t\n",
    "- asthmapct: percent of people in this zip code with athsma\n",
    "- popdens: population density in this zip code\t\n",
    "- medianage: median age in this zip code\t\n",
    "- housesize: average number of people per household in this zipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the covid data into a dataframe and get the descriptive statistics for each borough in the dataset. \n",
    "\n",
    "Hint: use groupby() with 'borough'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, get descriptive stats for each borough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the unique() method to find out which possible values appear in the 'borough' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the dataframe groupby() method to group the dataframe by `borough`, and then use the `get_groups()` method to pull the data for one of the boroughs that you choose.\n",
    "    \n",
    "You can use it like this:\n",
    "\n",
    "    \n",
    "    \n",
    "```\n",
    "group_object = df.groupby('col_name_to_group_by')\n",
    "one_group_df = group_object.get_group('name of desired group')\n",
    "```\n",
    "    \n",
    "or in one line:\n",
    "```\n",
    "one_group_df = df.groupby('col_name_to_group_by').get_group('name of desired group')\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for one borough using get_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataframe for one borough that you just created, make a scatterplot showing the relationship between medianincome and deathper100k columns. \n",
    "\n",
    "Your plot should include a title that tells us which Borough you selected for your analysis group.\n",
    "\n",
    "You can add a title to your plot by putting this after the seaborn plot command:\n",
    "\n",
    "```python\n",
    "plt.title('your title')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot medianincome vs deathper100k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the correlation between medianincome and deathper100k within your selected borough. What is the r value, what is the p value?\n",
    "\n",
    "In class we used scipy.stats to compute pearsonr().\n",
    "\n",
    "Pingouin also has a nice correlation function that can be used like this:\n",
    "\n",
    "```python\n",
    "import pingouin as pg\n",
    "corr_results = pg.corr(x=df['col1'], y = df['col2'])\n",
    "```\n",
    "\n",
    "That would compute the correlation between col1 and col2 in a dataframe called df. The results object is dataframe with the r value, p value, and other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute t-test\n",
    "\n",
    "For this problem you'll use the overall covid dataframe you loaded at the beginning of this section (the one with all the boroughs in it). \n",
    "    \n",
    "We'd like to determine if the rate of covid deaths (`deathper100k`) differed between Manhattan and Brooklyn. \n",
    "\n",
    "Conduct a t-test to answer this question. You will need to get the death rate data for these two boroughs into their own columns. There are several ways to do this (including one that you've seen in this notebook).\n",
    "    \n",
    "Report whether there is a difference in deathper100k rates in these two boroughs.\n",
    "\n",
    "In addition to the scipy t-test functions we learned about in class, Pingouin provides some t-test functions that provide more information in the results.\n",
    "\n",
    "To run an independent sample t-test with Pingouin you do this:\n",
    "\n",
    "```python\n",
    "t_results = pg.ttest(x=df['some_column'], y = df2['some_column'], correction=True)\n",
    "```\n",
    "\n",
    "Where x= and y= point to arrays of numbers to compare and the correction= argument is for Welch's test if the variances are unequal. From the pg.ttest() documentation:\n",
    "\n",
    "_\"**correction:** string or boolean<br>\n",
    "For unpaired two sample T-tests, specify whether or not to correct for unequal variances using Welch separate variances T-test. If ‘auto’, it will automatically uses Welch T-test when the sample sizes are unequal, as recommended by Zimmerman 2004.\"_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make a bar plot that shows the mean deathper100k rate within each borrow using sns.catplot() or another tool of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on the Covid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to see if we can predict how death rates vary across our measured zipcodes by using the other information we have for those zipcodes.\n",
    "    \n",
    "Conduct a multiple regression trying to predict deathper100k based on three other variables in the dataset. The other variables should be numeric and you can take your pick of which you think would be relevant for the prediction.\n",
    "    \n",
    "In your answer, indicate whether the estimated coefficient for each predictor variable is significantly different from zero.\n",
    "    \n",
    "You can use the ols() function implemented in statsmodels.formula.api\n",
    "    \n",
    "In the class examples we import statsmodels.formula.api as smf and then use smf.ols() to set up and fit the regression model.\n",
    "\n",
    "More examples of multiple regression are in the viz_and_stats_regression_inclass_April5th.ipynb and viz_and_stats_regression_inclass_April12th.ipynb notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounce data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will be analyzing a dataset that looks at the \"bounce times\"\n",
    "of users of a website with cooking recipes. The bounce time is a measure of how quickly someone leaves a website, e.g. the number of seconds after which a user first accesses a webpage from the website and then leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset in bounce.csv . The datatset has one row for each participant in the study. You should load the data into a dataframe called `bounce_data` and report the total number of people we have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a histogram of the distribution of bounce times in the dataset. \n",
    "\n",
    "Your plot should include a title, and the title should indicate what it is that you are plotting as well as the mean of the bounce time values. \n",
    "\n",
    "You should store the mean of the bounce time values in a variable and then concatenate that with the rest of your title. \n",
    "\n",
    "In other words, your code should work for any dataset of the same structure; you should not compute the mean and then enter that number directly into the figure title.\n",
    "\n",
    "\n",
    "**Hint**: dataframes have a .mean() method attached to them that will return the mean of all numeric columns in the data. The .mean() method returns a **Series**, and to get the value for one element in the Series you can ask for it by name: \n",
    "```# get the mean for all the columns:\n",
    "results = df.mean()\n",
    "# extract the mean for one column or variable:\n",
    "one_mean = results[<desired variable/column name>]\n",
    "\n",
    "# or you can do the above but all in one line\n",
    "one_mean = df.mean()[<desired variable/column name>]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Hint**: you can make a title for a seaborn figure by putting plt.title() after your plotting code. The plt.title() function takes a string as input and puts that string as a figure title. In order to access that library you will need to import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a scatterplot of bounce times versus age snd inspect the plot.\n",
    "We are interested in whether bounce times can be related to the age of the user.  Does it look there is a relationship between age and bounce time? Do older or younger people spend more time on the website?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantify the relationship proposed in the last scatterplot question: compute a regression that will tell us whether bounce time can be predicted by age. \n",
    "    \n",
    "After running the regression you should report the estimated coefficient for age and whether that coefficient is significantly different from zero. \n",
    "\n",
    "(reminder: after setting up and fitting your model, there is a .summary() method attached to the model fit object and that will give you all the info you need)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the best fit regression line\n",
    "\n",
    "The standard equation for a line is of the form \n",
    "\n",
    "$y = Intercept + w*x$\n",
    "\n",
    "Where x is each input value. \n",
    "    \n",
    "The regression results from the last question give you all the information you need to produce the best fitting regression line relating age to bounce time: if we ask for the summary() from the fit object we get an intercept from the model, and we have a coefficient that corresponds to the $w$ in our $w*x$ above.\n",
    "\n",
    "In this problem you will replot your age X bounce scatterplot from before but this time overlaying the best fit line. \n",
    "\n",
    "You can do that by first calling your seaborn plot and then calling matplotlib.pyplot to plot the line:\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# do your scatterplot:\n",
    "sns.whatever_kind_of_plot(whatever inputs)\n",
    "\n",
    "# add a line by specifying a set of x values and y values to pass to plt.plot():\n",
    "plt.plot(x_vals, y_vals)\n",
    "```\n",
    "\n",
    "To complete this problem you will need to generate a set of y_vals by re-writing the equation for a line and including the relevant intercept and w values from your regression model. You can get a bunch of y values all at once by applying your code to an array of x values. \n",
    "    \n",
    "    You can use `x_vals = np.linspace(0,100)` to get an array of x values from zero to 100. Then, if you did something like this, where b1 is the value for the regression coefficient for your predictor and intercept is the Intercept, you would get a bunch of y values based on the coefficient:\n",
    "    \n",
    "    b1 = .45\n",
    "    intercept = 10.1\n",
    "    x_vals = np.linspace(0,100)\n",
    "    \n",
    "    y_pred = (b1*x_vals) + intercept\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating data\n",
    "\n",
    "In this section we will take advantage of the random selection functions provided by numpy.\n",
    "\n",
    "```\n",
    "import numpy.random as npr\n",
    "\n",
    "# randomly select N numbers from a Gaussian normal distribution with mean mu and standard deviation sd\n",
    "mu = 0\n",
    "sd = 1\n",
    "N = 10\n",
    "npr.normal(mu, sd, N)\n",
    "\n",
    "# randomly select N numbers from a uniform distribution between min (inclusive) and max (not included)\n",
    "min = 0\n",
    "max = 100\n",
    "npr.uniform(min, max, N)\n",
    "\n",
    "# randomly select N INTEGERS from a uniform distribution between min (inclusive) and max (not included)\n",
    "min = 0\n",
    "max = 100\n",
    "npr.randint(min, max, N)\n",
    "\n",
    "\n",
    "# you can use np.mean() to get the average of a set of samples\n",
    "all_samples = npr.normal(mu, sd, N)\n",
    "all_samples_mean = np.mean(all_samples)\n",
    "\n",
    "# or in one line:\n",
    "all_samples_mean = np.mean(npr.normal(mu, sd, N))\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 10 samples from a Gaussian normal distribution with mean and sd of your choice. Make a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try taking samples of 25, 50, 100, 500, 1000, and 10000 and plotting the histogram. What happens to the shape as the sample size goes up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 1000 samples from a uniform distribution with min 0 and max 100. Histogram the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random samples in a loop\n",
    "\n",
    "Random number samples can be variable, and although you might be drawing data from a true normal distribution using npr.normal(), by chance you might sometimes get extreme values.\n",
    "    \n",
    "Build a loop that will run 1000 times. \n",
    "\n",
    "Each time through the loop, do a new sample of 25 values from a normal distribution with mean 0 and SD of 1. Then calculate the mean of those values using np.mean(x) where x is the set of 25 random samples.\n",
    "\n",
    "Store (append) the mean from each iteration of your loop in a list variable. You will want to make an empty list before the loop and then append the mean values to that.\n",
    "\n",
    "When you're done, histogram your 1000 mean values. You have now generated the \"sampling distribution of the mean\" which gives a sense of how often you would expect to observe sample means of various values give your sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
