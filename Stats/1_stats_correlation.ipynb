{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "\n",
    "This chapter is adapted from Matthew Crump's excellent [Answering questions with data](https://crumplab.github.io/statistics/) book.  The text has mainly been left intact with a few modifications, also the code adapted to use python and jupyter.\n",
    "\n",
    "\n",
    "\n",
    ">\"Correlation does not equal causation.\" ---Every statistics and research methods instructor ever\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    \n",
    "### NOTE: this notebook loads data from a file called 'WHR2018.csv' that is in Brightspace / Content / Data /\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pingouin as pg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats packages\n",
    "\n",
    "The core python library providing stats (and many other tools) is scipy. In addition, we will use pingouin, which is built on top of scipy and provides an easier to use interface for some stats tests and well as sometimes providing more useful results output than scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation background\n",
    "\n",
    "##### Understanding the relationship between measurements\n",
    "\n",
    "One of the fundamental ways we try to use data is to measure the relationship between measurements of different kinds. For example, does socioeconomic status have a relationship with academic outcomes?\n",
    "\n",
    "When we have two variables, each with a number of matched observations, we can ask whether the two variables are **correlated**. \n",
    "\n",
    "Although correlation does not equal causation, causation should be reflected in correlation, and one of our main goals in Psychology is to use theory and data to understand the causal relationship between proposed constructs. In this section we will warm up with covariance, correlation, and how to assess these statistics in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charlie and the Chocolate factory\n",
    "\n",
    "Imagine the following:\n",
    "\n",
    "- a person's supply of chocolate has a causal influence on their level of happiness\n",
    "\n",
    "- the more chocolate you have the more happy you will be, and the less chocolate you have, the less happy you will be\n",
    "\n",
    "- we suspect happiness is caused by lots of other things in a person's life, we anticipate that the relationship between chocolate supply and happiness won't be perfect\n",
    "\n",
    "What do these assumptions mean for how the data should look?\n",
    "\n",
    "Our first step is to collect some imaginary data from 100 people:  \n",
    "\n",
    "1. how much chocolate do you have\n",
    "2. how happy are you. \n",
    "\n",
    "For convenience, both the scales will go from 0 to 100. \n",
    "\n",
    "- chocolate: 0 means no chocolate, 100 means lifetime supply of chocolate. Any other number is somewhere in between. \n",
    "\n",
    "- happiness: 0 means no happiness, 100 means all of the happiness, and in between means some amount in between.\n",
    "\n",
    "Here is some sample data from the first 10 imaginary subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use several numpy (imported as np) functions to help us generate random behavior.\n",
    "\n",
    "`np.arange()`  gives us a range of numbers between a start and a stop value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = np.arange(1,101)\n",
    "subject[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy package includes a subpackage called `numpy.random`. We import that as `npr` to simplify accessing the random functions.\n",
    "\n",
    "`import numpy.random as npr`\n",
    "\n",
    "\n",
    "The `npr.uniform()` function gives us a random number(s) between a min and a max value, and all values are equally likely, or have _uniform_ probability of being sampled:\n",
    "\n",
    "`npr.uniform(min, max, n_samples)`\n",
    "\n",
    "If you leave the `n_samples` argument out you'll get one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get three random numbers between 1 and 5\n",
    "print(npr.uniform(1,5,3))\n",
    "\n",
    "# get one random number between 1 and 5\n",
    "print(npr.uniform(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 1000 samples between .5 and 1\n",
    "lots_of_random_numbers = npr.uniform(0.5, 1, 100000)\n",
    "\n",
    "# make a histogram of the data\n",
    "sns.displot(lots_of_random_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that whereas we previously used seaborn (sns) to plot data from dataframes, you can also use numpy arrays directly as the input the way that we did in this previous histogram example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.round()` function takes in a number (or an array of numbers), and an optional argument for how many decimal places to round to. By default it rounds to zero decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round to no decimal places by leaving the decimals= argument out\n",
    "np.round(1.2876)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round to four decimal places\n",
    "np.round(1.2876, decimals = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use some of those tools to make some fake data about chocolate consumption and happiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a set of participant numbers, 1-100:\n",
    "participants = np.arange(1,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a bunch of 'chocolate' measurements by getting \n",
    "# a bunch of random numbers between .5 and 1 and mutiply them\n",
    "# times a range of numbers going from 0-99.\n",
    "#\n",
    "# This will give us a set of increasing numbers with \n",
    "# some variability. the arange() gives us linear increase with no noise\n",
    "# and multiplying times the uniform .5 to 1 gives some jitter to make\n",
    "# the relationship not be a perfect correlation\n",
    "\n",
    "# make some chocolate scores\n",
    "chocolate = np.round(np.arange(100)*npr.uniform(0.5, 1, 100))\n",
    "\n",
    "# plot the chocolate scores as y values vs their\n",
    "# index position (np.arange(100))\n",
    "sns.relplot(x=np.arange(100), y=chocolate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same thing to get some happiness data as well\n",
    "happiness = np.round(np.arange(100)*npr.uniform(0.5, 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# use a dictionary to make a dataframe with columns subject, chocolate, and happiness\n",
    "# we'll set the index to be the 'participant' column:\n",
    "df_CC = pd.DataFrame({'participant': participants, \n",
    "                     'chocolate': chocolate, \n",
    "                     'happiness': happiness})\n",
    "\n",
    "df_CC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We asked each subject two questions so there are two scores for each subject, one for their chocolate supply, and one for their level of happiness. \n",
    "\n",
    "To look at the relationship between these scores we can plot them.\n",
    "\n",
    "### Scatter plots\n",
    "\n",
    "When you have two measurements worth of data, you can always turn them into in a scatter plot. \n",
    "\n",
    "A scatter plot has a horizontal x-axis, and a vertical y-axis. You choose which measurement goes on which axis. Let's put chocolate supply on the x-axis, and happiness level on the y-axis. \n",
    "\n",
    "We will use seaborn (imported as sns) scatterplot to look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# note the use of method 'chaining' to \n",
    "# attach .set_title() to the end of the \n",
    "# scatterplot call\n",
    "sns.scatterplot(x='chocolate', \n",
    "                y='happiness', \n",
    "                data=df_CC).set_title('Scatterplot of happiness versus chocolate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dot is for one person: 100 people, 100 dots. \n",
    "\n",
    "Each dot has an x-coordinate for chocolate and a y-coordinate for happiness.\n",
    "\n",
    "You can look at any dot, then draw a straight line down to the x-axis: that will tell you how much chocolate that subject has. You can draw a straight line left to the y-axis: that will tell you how much happiness the subject has.\n",
    "\n",
    "In this plot happiness is lower for people with smaller supplies of chocolate, and higher for people with larger supplies of chocolate. This kind of relationship is called a **positive correlation**. \n",
    "\n",
    "### Positive, Negative, and No-Correlation\n",
    "\n",
    "To simulate a negative relationship we will take advantage of a numpy.arange() feature that lets us get a range of numbers in reverse order by specifiying the \"step size\" to be negative.\n",
    "\n",
    "Numbers increasing from 0 to 100 in steps of 1:\n",
    "np.arange(0,100)\n",
    "\n",
    "Numbers descending from 100 to 0 in steps of 1:\n",
    "np.arange(100,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10,0,-.876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell has a bunch of code that simply makes a positive correlation, a negative correlation, and a random correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a positive relationship like we already did\n",
    "subject=np.arange(100)\n",
    "chocolate=np.round(np.arange(100)*npr.uniform(0.5, 1, 100))\n",
    "happiness=np.round(np.arange(100)*npr.uniform(0.5, 1, 100))\n",
    "df_CC_pos= pd.DataFrame({'subject': subject, \n",
    "                         'chocolate': chocolate, \n",
    "                         'happiness': happiness})\n",
    "\n",
    "\n",
    "# make a negative relationship by making the happiness data step\n",
    "# from 100 to using arange() with step size -1 (see below)\n",
    "subject=np.arange(100)\n",
    "chocolate=np.round(np.arange(100)*npr.uniform(0.5, 1, 100))\n",
    "# use arange negative step size to go from 100 down to 0\n",
    "happiness=np.round(np.arange(100,0,-1)*npr.uniform(0.5, 1, 100))\n",
    "df_CC_neg= pd.DataFrame({'subject': subject, \n",
    "                         'chocolate': chocolate, \n",
    "                         'happiness': happiness})\n",
    "\n",
    "# make a random relationship by just getting random numbers\n",
    "# 0 to 100 for chocolate and happiness\n",
    "subject=np.arange(100)\n",
    "chocolate=np.round(npr.uniform(0, 100, 100))\n",
    "happiness=np.round(npr.uniform(0, 100, 100))\n",
    "df_CC_zero= pd.DataFrame({'subject': subject, \n",
    "                          'chocolate': chocolate, \n",
    "                          'happiness': happiness})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using matplotlob to make subplots of our pos, neg, and random correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# this first part uses maplotlib (plt) to set up a figure with three subplot panels\n",
    "# the three subplot panels are the axes of the figure\n",
    "# the 1,3 means 1 row of panels and three columns\n",
    "fig, ax = plt.subplots(1, 3, \n",
    "                       figsize=(12, 4), \n",
    "                       sharex='all', \n",
    "                       sharey='all')\n",
    "\n",
    "# The ax variable is a handle, or pointer, to the individual axes \n",
    "# (reminder: axes are the individual plots) and \n",
    "# gives us a way to put data into each of the three axes \n",
    "# when we make the plots\n",
    "\n",
    "\n",
    "# tell seaborn to put the scatter plot into the first subplot by\n",
    "# including the ax= argument to scatterplot(). the ax variable has three \n",
    "# entries, one for each subplot panel\n",
    "sns.scatterplot(x='chocolate', \n",
    "                y='happiness', \n",
    "                data=df_CC_pos, \n",
    "                ax=ax[0]).set_title('positive')\n",
    "\n",
    "# put the negative correlation into the second (middle) panel (ax[1])\n",
    "sns.scatterplot(x='chocolate', \n",
    "                y='happiness', \n",
    "                data=df_CC_neg, \n",
    "                ax=ax[1]).set_title('negative')\n",
    "\n",
    "# put the negative correlation into the third (right) panel (ax[2])\n",
    "sns.scatterplot(x='chocolate', \n",
    "                y='happiness', \n",
    "                data=df_CC_zero,\n",
    "                ax=ax[2]).set_title('random')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's r\n",
    "\n",
    "In past weeks we did descriptive statistics for a single measure, like chocolate, or happiness (i.e., means, variances, etc.).\n",
    "\n",
    "A statistic that summarizes the relationship between two variables is \"Pearson's $r$\", and that's the r value that specifies the strength of a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Correlation scores range between -1 (perfect negative correlation) and +1 (positive correlation)\n",
    "\n",
    "Let's take a look at a formula for Pearson's $r$:\n",
    "\n",
    "$r = \\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}} = \\frac{cov(X,Y)}{SD_{X}SD_{Y}}$\n",
    "\n",
    "$\\sigma$ is often used as a symbol for the standard deviation (SD). In words, $r$ is the co-variance of X and Y, divided by the product of the standard deviation of X and the standard deviation of Y. This operation has the effect of **normalizing** the co-variance into the range -1 to 1. \n",
    "\n",
    "The formula for the co-variance is:\n",
    "\n",
    "$cov(X,Y) = \\frac{\\sum_{i}^{n}(x_{i}-\\bar{X})(y_{i}-\\bar{Y})}{N}$\n",
    "\n",
    "\n",
    "\n",
    "> It's worth saying that there are other formulas for computing Pearson's $r$. You can find them by Googling them. They will all give the same answer but vary in how it's calculated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with Data\n",
    "\n",
    "Let's look at some data from the [world happiness report](http://worldhappiness.report) (2018).\n",
    "\n",
    "Data are in a csv file ('WHR2018.csv') that should be placed in the same folder as this notebook in order to read it into pandas. Or, make sure you include the **path** to the file along with the filename if you save it somewhere else.\n",
    "\n",
    "\n",
    "This report measured various attitudes across people from different countries. \n",
    "\n",
    "For example, one question asked about how much freedom people thought they had to make life choices. Another question asked how confident people were in their national government. \n",
    "\n",
    "Here is a scatterplot showing the relationship between these two measures. Each dot represents means for different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to read the csv file and store it as a dataframe in the variable whr_df\n",
    "whr_df = pd.read_csv('../../../data/stats_data/WHR2018.csv', sep = ',')\n",
    "whr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of the columns in the original dataframe, \n",
    "# and use .dropna() to get rid of any rows \n",
    "# in the dataframe that are missing data for some measurement\n",
    "\n",
    "# list of column names we want to keep in the df\n",
    "cols_to_keep = ['country', \n",
    "                'Freedom to make life choices', \n",
    "                'Confidence in national government']\n",
    "\n",
    "# make a smaller dataframe using only our\n",
    "# desired columns\n",
    "# We simply pass a list of columns we want\n",
    "smaller_df = whr_df[cols_to_keep].copy()\n",
    "smaller_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the dataframe dropna() function with argument inplace=True \n",
    "# to do the change in the existing dataframe (rather than outputting to a new variable):\n",
    "smaller_df.dropna(inplace=True)\n",
    "smaller_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# use seaborn regplot() to make a scatterplot but also \n",
    "# include the regression line showing the best fitting\n",
    "# line mapping x values to y values\n",
    "# regplot() follows the conventional seaborn structure\n",
    "# where we can pass dataframe data by specifying column names\n",
    "# for x= and y= and then point data= at the dataframe variable\n",
    "\n",
    "# the line_kws and scatter_kws are options to change the color of the\n",
    "# regression line to blue and change the dots to size = 2 and color = black\n",
    "sns.regplot(x='Freedom to make life choices', \n",
    "            y='Confidence in national government', \n",
    "            data=smaller_df,\n",
    "            line_kws={'color':'steelblue'}, \n",
    "            scatter_kws={'s': 2, 'color': 'black'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put a blue line on the scatterplot to summarize the positive relationship.\n",
    "\n",
    "The actual correlation, as measured by Pearson's $r$ can be obtained using pinguoin or scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation using pinguoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pinguoin (imported as pg) to compute the correlation\n",
    "# it takes in a set of x values, a set of y values, and the type of correlation\n",
    "corr_results = pg.corr(x=smaller_df['Freedom to make life choices'], \n",
    "                       y=smaller_df['Confidence in national government'], \n",
    "                       method='pearson')\n",
    "\n",
    "corr_results['r'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pingouin corr() function returns a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(corr_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access a column of the dataframe we simply do\n",
    "# corr_results[column_name]\n",
    "#\n",
    "# doing that will return a pandas Series\n",
    "# To get access to the actual values we do:\n",
    "# df[column_name].values\n",
    "# and that will give us a numpy array\n",
    "# which we can index like a list:\n",
    "print(corr_results['r'].values)\n",
    "print(corr_results['r'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can dynamically populate a little stats reporting text like this:\n",
    "\n",
    "# get the degrees of freedom by getting sample size minus 2\n",
    "deg_free = corr_results['n'][0]-2\n",
    "\n",
    "# get the r value\n",
    "r = corr_results['r'][0]\n",
    "# round it to two decimal places\n",
    "r = np.round(r,2)\n",
    "\n",
    "# get the p value\n",
    "p = corr_results['p-val'][0]\n",
    "\n",
    "# the p-value for this test is super small (4.1e-57)\n",
    "# this is scientific notation and means move the decimal places\n",
    "# 57 places to the left!\n",
    "\n",
    "# let's check the magnitude of the p value and adjust our stats reporting accordingly:\n",
    "if p < .0001:\n",
    "    p_text = 'p < .0001'\n",
    "    sig = True\n",
    "\n",
    "elif p < .001:\n",
    "    p_text = 'p < .001'\n",
    "    sig = True\n",
    "elif p < .05:\n",
    "    p_text = 'p < .05'\n",
    "    sig = True\n",
    "    \n",
    "# if p value is not significant at one of the levels\n",
    "# we can just print it out, rounded to two places:\n",
    "else:\n",
    "    p_text = f'p = {np.round(p, 2)}'\n",
    "    sig = False\n",
    "\n",
    "\n",
    "# report the correlation based on whatever the results were\n",
    "var1 = 'Freedom to make life choices'\n",
    "var2 = 'Confidence in government'\n",
    "if sig:\n",
    "    stats_text = f'The correlation between {var1} and {var2} was significant (r({deg_free}) = {r}, {p_text}.)'\n",
    "\n",
    "\n",
    "print(stats_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to keep in mind when we think about correlation and the relationship between variables. Looking at the graph you might start to wonder: Does freedom to make life choices cause changes how confident people are in their national government? Our does it work the other way? Does being confident in your national government give you a greater sense of freedom to make life choices? Or, is this just a random relationship that doesn't mean anything? All good questions. These data do not provide the answers, they just suggest a possible relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation using scipy\n",
    "\n",
    "\n",
    "scipy.stats library includes a pearonr() function that takes in two sets of numbers to compute the pearson's r value for.\n",
    "\n",
    "It returns a \"tuple\" which is like a list. The first entry is the r value and the second is the p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scipy_corr = stats.pearsonr(smaller_df['Freedom to make life choices'], \n",
    "                            smaller_df['Confidence in national government'])\n",
    "\n",
    "# get the r value\n",
    "print(scipy_corr[0])\n",
    "\n",
    "# get the p value\n",
    "print(scipy_corr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice thing about pingouin vs scipy is that pingouin gives you some additional info with the p and r values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Correlations\n",
    "\n",
    "- #### Correlation does not equal causation\n",
    "\n",
    "- #### And even when there is causation there might no be obvious correlation\n",
    "\n",
    "Consider buying a snake plant for your home. Like most plants, snake plants need some water to stay alive. However, they also need just the right amount of water. \n",
    "\n",
    "Imagine an experiment where 1000 snake plants were grown in a house. Each snake plant is given a different amount of water per day, from zero teaspoons of water per day to 1000 teaspoons of water per day. \n",
    "\n",
    "We will assume that water is part of the causal process that allows snake plants to grow. \n",
    "\n",
    "The amount of water given to each snake plant per day can of our measures. Every week the experimenter measures snake plant growth, which will be the second measurement. \n",
    "\n",
    "Now, can you imagine for yourself what a scatter plot of weekly snake plant growth by tablespoons of water would look like?\n",
    "\n",
    "The first plant given no water at all would have a very hard time and eventually die. It should have the least amount of weekly growth. \n",
    "\n",
    "The plants given only a few teaspoons of water per day could get just enough water to keep the plants alive, so they will grow a little bit but not a lot. \n",
    "\n",
    "As we look at snake plants getting more and more water, we should see more and more plant growth, but only up to a point. Too much water can be bad for these plants. \n",
    "\n",
    "Data like this will produce a scatter plot with an upside down U shape.\n",
    "\n",
    "Computing Pearson's $r$ for data like this can give you $r$ values close to zero. The scatter plot could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# np.linspace() takes a start and stop value and the number of desired steps\n",
    "# and returns to you a set of numbers\n",
    "# here we are asking for 1000 evenly spaced values between 0 and 1000\n",
    "water = np.linspace(0,1000,1000)\n",
    "\n",
    "# concatenate a range of numbers going from 0 to 10 and then a set going from 10 to 0\n",
    "growth = np.concatenate(\n",
    "    (np.linspace(0,10,500), \n",
    "     np.linspace(10,0,500)), \n",
    "    axis=None)\n",
    "\n",
    "# randomly choose 1000 values from uniform distribution between -2 and 2\n",
    "noise = npr.uniform(-2,2,1000)\n",
    "\n",
    "# add some \"noise\" or randomness to the growth variable\n",
    "growth = growth+noise\n",
    "\n",
    "# make dataframe from dictionary:\n",
    "snake_df = pd.DataFrame({\"growth\": growth, \n",
    "                         \"water\": water})\n",
    "\n",
    "sns.scatterplot(x='water', \n",
    "                y='growth',\n",
    "                data=snake_df).set_title('Imaginary snake plant growth as a function of water')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a relationship between watering and snake plant growth. But, the correlation isn't in one direction. As a result, when we compute the correlation in terms of Pearson's r, we get a value suggesting no relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_stats = pg.corr(snake_df['growth'], snake_df['water'])\n",
    "\n",
    "\n",
    "corr_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an r value close to 0, and a p value way greater than .05.\n",
    "\n",
    "There is no linear relationship that can be described by a single straight line. When we need lines or curves going in more than one direction, we have a nonlinear relationship.\n",
    "\n",
    "This example illustrates some conundrums in interpreting correlations. \n",
    "\n",
    "> Pro Tip: This is one reason why plotting your data is so important. If you see an upside U shape pattern, then a correlation analysis is probably not the best analysis for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Random chance\n",
    "\n",
    "Another very important aspect of correlations is the fact that they can be produced by random chance. This means that you can find a positive or negative correlation between two measures, even when they have absolutely nothing to do with one another. These are **spurious** correlations, produced just by chance alone. \n",
    "\n",
    "Imagine a situation with no causal connection:\n",
    "\n",
    "- two participants\n",
    "- one at north pole with a lottery machine full of balls with number from 1 to 10\n",
    "- one at the south pole with a similar machine\n",
    "- each participant randomly chooses 10 balls and records the number\n",
    "\n",
    "\n",
    "### Simulating a bunch of random correlation analyses:\n",
    "Here is what the numbers on each ball could look like for each participant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose 10 numbers between 1 and 10\n",
    "north_pole = np.round(npr.uniform(1,10,10))\n",
    "\n",
    "# choose 10 more random numbers between 1 and 10\n",
    "south_pole = np.round(npr.uniform(1,10,10))\n",
    "\n",
    "# make dataframe from dictionary\n",
    "df_poles = pd.DataFrame({'north_pole': north_pole, \n",
    "                         'south_pole': south_pole})\n",
    "\n",
    "df_poles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# use pingouin to compute the correlation\n",
    "results = pg.corr(x=df_poles['north_pole'], \n",
    "                  y=df_poles['south_pole'])\n",
    "\n",
    "# print the pearson's r value\n",
    "results['r'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     " r ": "<p><strong>NameError</strong>: name &#39;r&#39; is not defined</p>\n"
    }
   },
   "source": [
    "In this one case, if we computed Pearson's $r$, we would find that $r =$ something. \n",
    "\n",
    "But, we know that relationship should be completely random, because that is how we set up the game.\n",
    "\n",
    "The question is what can random chance do? If we ran our game over and over again thousands of times, each time choosing new balls, and each time computing the correlation, what would we find?\n",
    "\n",
    "First, we will find fluctuation. The r value will sometimes be positive, sometimes be negative, sometimes be big and sometimes be small. \n",
    "\n",
    "Second, we will see what the fluctuation looks like. This will give us a window into the kinds of correlations that chance alone can produce. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# empty list to append to for \n",
    "# keeping track of simulated\n",
    "# results\n",
    "simulated_correlations = []\n",
    "\n",
    "# do 1000 simulations of randomly shuffling\n",
    "# ten numbers and computing the correlation\n",
    "for sim in range(1000):\n",
    "    north_pole = np.round(npr.uniform(1,10,10))\n",
    "    south_pole = np.round(npr.uniform(1,10,10))\n",
    "\n",
    "    # use scipy.stats pearson r\n",
    "    r, p = stats.pearsonr(north_pole,south_pole)\n",
    "    \n",
    "    # keep track of the r value for this loop of the simulation\n",
    "    simulated_correlations.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn scatterplot, passing arrays of numbers to \n",
    "# x and y arguments (not using dataframe)\n",
    "# we will also keep track of the handle or pointer\n",
    "# to the plot in variable pl. This will be used to set labels\n",
    "# for the x and y axis\n",
    "\n",
    "# plot the correlation values against an index of 0-999\n",
    "pl = sns.relplot(x=np.arange(1000), \n",
    "            y=simulated_correlations)\n",
    "\n",
    "# add labels to x and y axis\n",
    "# we have to do this because we didn't\n",
    "# use a dataframe for the relplot() above\n",
    "pl.set_axis_labels('sims', 'simulated correlation value')\n",
    "\n",
    "# use matplotlib to add green lines at y position 1 and\n",
    "# y position -1\n",
    "\n",
    "# the firs input is x values (0, 1000) and the second\n",
    "# is y values\n",
    "# plt.plot() makes a line connecting points, so we are making\n",
    "# a line that connects the x,y point= (0, 1) to the x,y point (1000, 1)\n",
    "# and another line that connects (0, -1) and (1000, -1)\n",
    "plt.plot([0, 1000], [1, 1], linewidth=2, color='green')\n",
    "plt.plot([0, 1000], [-1, -1], linewidth=2, color='green')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dot in the scatter plot shows the Pearson $r$ for each simulation from 1 to 1000. All the dots in between the range -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look the distribution of r values we got in our simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn displot() makes a histogram\n",
    "# simulated correlations is an array of r values\n",
    "sns.displot(simulated_correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution plot makes clear that the bulk of our simulated correlations were close to zero, and there is increasingly smaller chance of randomly observing large positive or negative r values. The significance value of a correlation is related to these distributions, but note that the more correlations you run, the more chance you have to find a false positive result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important lesson here is that random chance produced all of these correlations. This means we can find \"correlations\" in the data that are completely meaningless, and do not reflect any causal relationship between one measure and another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Correlation and the close related scatterplot are useful tools for examining the relationship between two variables with matched observations.\n",
    "\n",
    "We can easily compute correlations using scipy.stats:\n",
    "\n",
    "```\n",
    "import scipys.stats as stats\n",
    "x = some numbers\n",
    "y = some numbers\n",
    "\n",
    "r,p = stats.pearsonr(x, y)\n",
    "```\n",
    "\n",
    "Or using Pingouin:\n",
    "\n",
    "```\n",
    "# using pingouin with arrays of numbers\n",
    "import pingouin as pg\n",
    "x = some numbers\n",
    "y = some numbers\n",
    "\n",
    "results = pg.corr(x, y, method='pearson')\n",
    "\n",
    "results['r'].values[0]\n",
    "results['p-val'].values[0]\n",
    "```\n",
    "\n",
    "```\n",
    "# using pingouin with a dataframe\n",
    "import pingouin as pg\n",
    "x = some numbers\n",
    "y = some numbers\n",
    "\n",
    "df = pd.DataFrame({'var1': x, 'var2': y})\n",
    "\n",
    "results = pg.corr(x=df['var1'], y=df['var2'], method='pearson')\n",
    "\n",
    "results['r'].values[0]\n",
    "results['p-val'].values[0]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other kinds of correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course not a complete walkthrough of all things correlation in Python. In particular we only looked at computing pearson's R and there other forms of correlation that one can compute. Pinguoin provides easy access to these other calculations using the `method=` argument with one of the following:\n",
    "\n",
    "\n",
    "Correlation type:\n",
    "\n",
    "'pearson': Pearson ùëü product-moment correlation\n",
    "\n",
    "'spearman': Spearman ùúå rank-order correlation\n",
    "\n",
    "'kendall': Kendall‚Äôs ùúèùêµ correlation (for ordinal data)\n",
    "\n",
    "'bicor': Biweight midcorrelation (robust)\n",
    "\n",
    "'percbend': Percentage bend correlation (robust)\n",
    "\n",
    "'shepherd': Shepherd‚Äôs pi correlation (robust)\n",
    "\n",
    "'skipped': Skipped correlation (robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scipy these different kinds of correlation can be computed using different functions:\n",
    "\n",
    "`scipy.stats.spearmanr`\n",
    "\n",
    "`scipy.stats.kendalltau`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
