{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression - Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Written by Todd Gureckis*<br>\n",
    "*Updated by Shannon Tubridy (2022)*\n",
    "\n",
    "Small introductory parts are adapted from Danielle Navarro's excellent [Learning Statistics with R](https://learningstatisticswithr.com) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals for this notebook\n",
    "\n",
    "This notebook continues viz_and_stats1.ipynb, introducing some additional Seaborn tools as well as more stats\n",
    "\n",
    "- Doing regression in Python\n",
    "    - single predictor variable\n",
    "    - multiple regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib is the core python plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn is a plotting library built on top of matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# scipy is a library with tons of scientific computing functions\n",
    "# the scipy.stats sub library has many common stats calculations in it\n",
    "import scipy.stats as stats\n",
    "\n",
    "# statsmodels is another stats library\n",
    "# it has good OLS regression functions that we'll use\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# pingouin is yet another stats library\n",
    "import pingouin as pg\n",
    "\n",
    "\n",
    "# note this next part\n",
    "# it is not an import\n",
    "# instead it is part of Jupyter\n",
    "# as denoted by the % sumbol in front\n",
    "# which you have already seen for the \n",
    "# %whos command\n",
    "# Here we are telling jupyter to make the\n",
    "# figures from matlplotlib to be displayed\n",
    "# after each cell\n",
    "%matplotlib inline\n",
    "\n",
    "# in later versions of jupyter notebook \n",
    "# this inline call isn't strictly necessary but it doesn't hurt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression assesses the extent to which a predictor variable or variable(s) influences the value of an outcome variable.\n",
    "\n",
    "The results of a linear regression analysis tell you:\n",
    "\n",
    "- how a change in a predictor variable leads to change in an outcome variable (the coefficients for each predictor)\n",
    "\n",
    "- how much of the variance in the outcome variable measurements can be accounted for by the variance in the predictor variable(s)\n",
    "\n",
    "For example:\n",
    "\n",
    "- Does the working memory capacity (independent or predictor variable) relate to long-term memory abilities (outcome variable)\n",
    "\n",
    "- Does the magnitude of environmental noise in a neighborhood (predictor) relate to degree of stress people experience as measured in cortisol levels (outcome)\n",
    "\n",
    "- Does the the quantity of gain or loss (predictor) in a decision making experiment affect neural activity in the dopamine system (outcome)\n",
    "\n",
    "- How do hours of caregiver interaction and parent's socioeconomic status (predictor variables) impact educational performance (outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we can look at the relationship between the amount of sleep a person gets each night and their mood the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the sleep dataset (sleep_mood.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains measurements for four variables corresponding to: \n",
    "\n",
    "- the nightly sleep the previous night (`sleep_hours`)\n",
    "- how many hours of work the previous day (`work_hours`)\n",
    "- the person's grumpiness level the next day (`next_day_grumpiness`)  \n",
    "- which day of the observation period we are referring to (`day`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review: make histograms showing the distribution of observations for the sleep_hours, work_hours, and next_day_grumpiness data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of analyzing the data, we have a hypothesis that the amount of sleep each night impacts the next day's grumpiness.\n",
    "\n",
    "A scatter plot will give us a quick view of the relationship of the data on those two measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a scatter plot showing parent sleep and parent grumpy for each day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the correlation between these two variables using scipy.stats pearsonr() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check the correlation between parent sleep and parent grumpy using scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation with Pingouin stats libary\n",
    "\n",
    "Pingouin is a stats library built on top of scipy that provides a good interface for many common statistical tests.\n",
    "\n",
    "The output is often more informative than that for scipy but there are some tests that are not implemented in Pingouin.\n",
    "\n",
    "Here is doing correlation with pingouin (imported as pg) corr() function.\n",
    "\n",
    "\n",
    "```python\n",
    "corr_results = pg.corr(x=one_set_of_numbers, \n",
    "        y=a_matched_set_of_other_numbers)\n",
    "```\n",
    "\n",
    "The output of pg.corr() is a dataframe with the r and p values as well as some additional info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do parent sleep X parent grumpy correlation with pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn regplot()\n",
    "\n",
    "Regression goes beyond correlation in specifying the nature of the relationship between a predictor and outcome variable and in particular the magnitude of the change in one variable given change in the other. This relationship can be summarized in the regression line that maps X data to a predicted Y value.\n",
    "\n",
    "Seaborn has a built in plot type called regplot() that is like a scatter plot that puts the best fitting regression line through the data.\n",
    "\n",
    "Use it like this:\n",
    "\n",
    "```python\n",
    "sns.regplot(x=predictor_variable, \n",
    "            y=outcome_variable, \n",
    "            data=df)\n",
    "```\n",
    "\n",
    "Note that the data= argument is only necessary if x= and y= point to dataframe columns. If you have two sets of numbers on another format like lists or arrays you can just use the x= and y= arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make scatterplot of sleep and grumpiness with the best \n",
    "# fitting regression line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and best fit line\n",
    "\n",
    "There is much more info about regression background in the notebook called 3_regression_1.ipynb located in Brightspace under Content / Code / stats\n",
    "\n",
    "At the core of ordinary least squares regression is an equation of the form:\n",
    "\n",
    "$\n",
    "y = mx + c\n",
    "$ \n",
    "\n",
    "Where y is an outcome value, x is the predictor variable, m is the coefficient and c is the intercept.\n",
    "\n",
    "The game is to find values of m and c so that when we push each individual x value through the equation it is as close as possible to the actual outcome value for that particular x value.\n",
    "\n",
    "Finding m and c to minimize the _sum of squared errors_, or the squared difference between each predicted and true y value is what ordinary least square regression (OLS) does.\n",
    "\n",
    "The equation for a line can also be written like this:\n",
    "\n",
    "\n",
    "$\n",
    "\\hat{Y_i} = b_1 X_i + b_0\n",
    "$\n",
    "\n",
    "Where: \n",
    "\n",
    "$\\hat{Y_i}$ denotes the _i_th measurement in the outcome variable\n",
    "\n",
    "$X_i$ is the _i_th measurement in the predictor variable\n",
    "\n",
    "$b_1$ is the coefficient that we multiply times each x value\n",
    "\n",
    "$b_0$ is the intercept, or the value of the outcome variable when the predictor is zero\n",
    "\n",
    "\n",
    "For any values of $b_1$ and $b_0$ and a single X observation there will be a _predicted_ outcome value ($\\hat{Y_i}$) that can be compared to the actual outcome value associated with that X observation ($Y_i$).\n",
    "\n",
    "The difference between those values is our _residual error_\n",
    "\n",
    "$\n",
    "\\epsilon_i = Y_i - \\hat{Y}_i\n",
    "$\n",
    "\n",
    "This means that the regression model takes this form:\n",
    "\n",
    "$\n",
    "Y_i = b_1 X_i + b_0 + \\epsilon_i\n",
    "$\n",
    "\n",
    "The $\\epsilon$ symbol is the Greek letter epsilon. It's traditional to use $\\epsilon_i$ or $e_i$ to denote a residual.\n",
    "\n",
    "And the best fitting line is the that minimizes the sum of squared error terms. Specifically, we want to minimize this quantity:\n",
    "\n",
    "$\\sum_i (Y_i - \\hat{Y}_i)^2$ \n",
    "\n",
    "or, equivalently:\n",
    "\n",
    "$\\sum_{i=1}^{n}\\epsilon_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `statsmodels ols()` regression function\n",
    "\n",
    "There are a number of libraries that will perform linear regression in python.  These range from complex machine learning libraries (e.g., [scikit-learn](https://scikit-learn.org/stable/)) to simple statistics libraries (e.g., [pingouin](https://pingouin-stats.org)).  Here we will focus on the [statsmodels](https://www.statsmodels.org/stable/index.html) library because it is one that is most geared toward traditional statistical analysis.\n",
    "\n",
    "The ordinary least squares algorithm is accessible via the `statsmodel.smf` submodule.  To import it do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will provide access to a function `ols()` which will perform ordinary least squares regression. \n",
    "\n",
    "`ols()` is quite flexible. If you imported the library using the command above and type `smf.ols?`, the help files will reveal that there are a lot of arguments that you can specify. \n",
    "\n",
    "For now there are two inputs that we need to proceed:\n",
    "\n",
    "\n",
    "- `formula`. A formula that specifies the regression model. For the simple linear regression models that we've talked about so far, in which you have a single predictor variable as well as an intercept term, this formula is of the form _outcome ~ predictor_. However, more complicated formulas are allowed, and we'll discuss them later.\n",
    "- `data`. The data frame containing the variables.\n",
    "\n",
    "The output of the `smf.ols()` function is fairly complicated object, with quite a lot of technical information buried under the hood. \n",
    "\n",
    "Because this technical information is used by other functions, it's generally a good idea to create a variable that stores the results of your regression. \n",
    "\n",
    "To run a single variable linear regression, the command is this:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# First: define the linear model\n",
    "lr_model = smf.ols(formula=\"outcome_variable ~ predictor_variable\", data=data_frame_object)\n",
    "\n",
    "# Second: fit the model (get the best coefficients to minimize the residuals)\n",
    "lr =  lr_model.fit()\n",
    "```\n",
    "\n",
    "#### Note that if you specify the regression model using the formula= syntax then the OLS function will automatically add a constant predictor to your model (which is usually you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do OLS regression to find the best fit line of \n",
    "# grumpiness (outcome) based on amount of sleep (predictor)\n",
    "\n",
    "# fit the model (get the best coefficients to minimize the residuals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ols() call we defined our regression model and specified where the data are. \n",
    "\n",
    "The model is described in the formula \"next_day_grumpiness ~ sleep_hours\". \n",
    "\n",
    "This syntax is saying model next_day_grumpiness as a function of, or based on the values in, sleep_hours. \n",
    "\n",
    "`next_day_grumpiness` is the outcome variable and `sleep_hours` is the predictor variable. \n",
    "\n",
    "Later we will see multiple regression using more than one predictor.\n",
    "\n",
    "After defining the model we estimated the best fitting parameters using the `.fit()` method, or function, attached to our defined model.\n",
    "\n",
    "We could also do it in one line by _chaining_ the .fit() command onto the end of the sms.ols() call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chain ols() and fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we .fit() the model the regression results are in our `lr` variable.\n",
    "\n",
    "Lets focus on `lr.params` for a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at the OLS parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two separate pieces of information here. \n",
    "\n",
    "First python gives us the intercept $\\hat{b}_0 = 125.96$ and then the slope or coefficient associated with any predictor variables. \n",
    "\n",
    "In our case that is just the sleep variable $\\hat{b}_1 = -8.94$. \n",
    "\n",
    "In other words, the best-fitting regression line that seaborn plotted earlier has this formula:\n",
    "\n",
    "$\n",
    "\\hat{Y}_i = -8.94 \\ X_i + 125.96\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access the coefficients and intercept directly like this:\n",
    "\n",
    "lr.params['sleep_hours']\n",
    "\n",
    "lr.params['Intercept']\n",
    "\n",
    "Of course, if your predictor variable was called something else you would change the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the estimated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remember the definition of the slope of a line, a regression coefficient of $\\hat{b}_1 = -8.94$ means that if increase $X_i$ increases by 1, then $Y_i$ decreases by 8.94: each additional hour of sleep will improve mood, reducing grumpiness by 8.94 grumpiness points. \n",
    "\n",
    "The $\\hat{b}_0$ corresponds to \"the expected value of $Y_i$ when $X_i$ equals 0\". It implies that if this person gets zero hours of sleep ($X_i =0$) then grumpiness will go off the scale, to an insane value of ($Y_i = 125.96$). Very grumpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the results output to manually plot the best fitting line. We just need the parameters from the model and a set of x values. From that we generate a set of predicted Y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use seaborn to scatterplot the original data:\n",
    "\n",
    "\n",
    "# use numpy linspace to make 20 evenly spaced values from 3 to 10:\n",
    "\n",
    "\n",
    "# use the regression parameters to get estimated y values for each input x:\n",
    "\n",
    "\n",
    "\n",
    "# add the line to the scatter plot using matplotlib (plt.plot(x, y)):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that the regression model gives us a way to predict parent_grumpy levels for values of parent_sleep that have not actually been observed yet. \n",
    "\n",
    "This is a very powerful aspect of regression-based analysis. (but note that you should be cautious in making predictions for predictor variable values far outside the range of the fitted data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How grumpy would someone who get 3.5 hours of sleep be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS regression results that we stored in the variable `lr` has lots more information available to us and we can get a comprehensive output using the lr.summary() command. Later in this notebook we'll come back parse the output in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "The simple linear regression model that we've discussed up to this point assumes that there's a single predictor variable that you're interested in, in this case the hours of sleep a person got. \n",
    "\n",
    "In many research there are multiple predictors that you want to examine and we can do this with **_multiple regression_**. All we do is add more terms to our regression equation. \n",
    "\n",
    "Now we want to use both `sleep_hours` and `work_hours` to predict the `next_day_grumpiness` variable. \n",
    "\n",
    "As before, we let $Y_i$ refer to grumpiness on the $i$-th day. \n",
    "\n",
    "We'll let $X_{i1}$ refer to the hours the parent slept on the $i$-th day, and $X_{i2}$ refers to the hours that the baby slept on that day. Now we can write our regression model like this:\n",
    "\n",
    "$\n",
    "Y_i = b_2 X_{i2} + b_1 X_{i1} + b_0 + \\epsilon_i\n",
    "$\n",
    "\n",
    "As before, $\\epsilon_i$ is the residual associated with the $i$-th observation, $\\epsilon_i = {Y}_i - \\hat{Y}_i$. \n",
    "\n",
    "In this model, there are three coefficients to be estimated: $b_0$ is the intercept, $b_1$ is the coefficient associated with parent sleep, and $b_2$ is the coefficient associated with baby sleep. \n",
    "\n",
    "Although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients $\\hat{b}_0$, $\\hat{b}_1$ and $\\hat{b}_2$ are those that minimise the sum squared residuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple regression in Python\n",
    "\n",
    "Multiple regression in python is no different to simple regression: all we have to do is specify a more complicated `formula` when using the `smf.ols()` function. \n",
    "\n",
    "For example, if we want to use both `sleep_hours` and `work_hours` as predictors in our attempt to explain grumpiness, then the formula we need is this:\n",
    "```\n",
    "                    next_day_grumpiness ~ sleep_hours + work_hours\n",
    "```\n",
    "\n",
    "Notice that just like with single regression there is no reference to the intercept term in this formula; only the two predictor variables and the outcome, and ols() will add the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run a multiple regression to test parent and baby sleep as predictors\n",
    "# of parent grumpiness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the fit `.params` from this regression model to see the estimated regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check the multiple regression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use the estimated parameters to generate the best fitting line with two predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use the multiple regression coefficients to make a prediction equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the fit of the regression model\n",
    "\n",
    "So far we have estimated the coefficients of a linear regression model.\n",
    "\n",
    "The regression model only produces a prediction, $\\hat{Y}_i$, about what mood is like each day: the actual mood is $Y_i$. \n",
    "\n",
    "If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.\n",
    "\n",
    "### The $R^2$ value\n",
    "\n",
    "In regression, $R^2$ indicates the proportion of the total variance in the outcome that be accounted for by variability in the predictor(s). \n",
    "\n",
    "A perfect model would explain all of the variance in the outcome meaning that the actual and predicted Y values were the same and the sum of squared errors was zero. As a formula:\n",
    "\n",
    "$\n",
    "R^2 = 1 - \\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$\n",
    "\\mbox{SS}_{res} = \\sum_i (Y_i - \\hat{Y}_i)^2\n",
    "$ and \n",
    "$\n",
    "\\mbox{SS}_{tot} = \\sum_i (Y_i - \\bar{Y})^2\n",
    "$\n",
    "\n",
    "You have all the tools to calculate this yourself, but we can get it for free from the OLS model fit.\n",
    "\n",
    "The model fit object has a summary() method and it will report the R-squared value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or access the value directly in the .rsquared attribute of the model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: manually calculate $R^2$\n",
    "\n",
    "np.sum() \n",
    "- will give the sum of anything in the parentheses\n",
    "\n",
    "(x-y)**2 \n",
    "- will square the difference of x and y\n",
    "\n",
    "df[column_name].mean() \n",
    "- gives the mean of a column\n",
    "\n",
    "y_pred = lr.params[predictor_name]*df[predictor_name] + lr.params['Intercept'] \n",
    "- gives the best predictions for y values given the fit model and the actual predictor values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The adjusted $R^2$ value\n",
    "\n",
    "Sometimes people report a slightly different measure of model performance, known as \"adjusted $R^2$\". The motivation behind calculating the adjusted $R^2$ value is the observation that adding more predictors into the model will *always* cause the $R^2$ value to increase (or at least not decrease).\n",
    "\n",
    "Adjusted r-squared appears in the .summary() and can be accessed as the rsquared_adj attribute of the model fit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis tests for regression models\n",
    "\n",
    "So far we've talked about what a regression model is, how the coefficients are estimated, and how to quantify the peformance of the model.\n",
    "\n",
    "Next is hypothesis tests. There are two different (but related) kinds of hypothesis tests we'll discuss: \n",
    "\n",
    "1) testing whether the regression model as a whole is performing significantly better than a null model at accounting for the data\n",
    "\n",
    "2) testing whether a particular regression coefficient is significantly different from zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model as a whole\n",
    "\n",
    "The first hypothesis we'll discuss is one in which the null hypothesis that there is *no relationship* between the predictors and the outcome. \n",
    "\n",
    "Formally, our \"null model\" corresponds to the fairly trivial \"regression\" model in which we include 0 predictors, and only include the intercept term $b_0$.\n",
    "\n",
    "\n",
    "$\n",
    "H_0: Y_i = b_0 + \\epsilon_i\n",
    "$\n",
    "\n",
    "If our regression model has $K$ predictors, the \"alternative model\" is described using the usual formula for a multiple regression model:\n",
    "\n",
    "$\n",
    "H_1: Y_i = \\left( \\sum_{k=1}^K b_{k} X_{ik} \\right) + b_0 + \\epsilon_i\n",
    "$\n",
    "\n",
    "The test statistic for this test is the $F$ statistic which has exactly the same interpretation as it does in ANOVA. Large $F$ values indicate that the null hypothesis is performing poorly in comparison to the alternative hypothesis. You can find more details in the 3_stats_regression_1.ipynb stand-alone notebook.\n",
    "\n",
    "The F stat and associated p-value can be found in the regression fit summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output that this command produces is pretty dense, but we've already discussed a few parts of interest in it. \n",
    "\n",
    "It is a little hard to tell from the output, but there are three tables.  \n",
    "\n",
    "### Overall model information\n",
    "\n",
    "First it reminds us that what the dependent measure is.\n",
    "\n",
    "Next we see the **R-squared** and **Adj. R-squared** which are the two versions of the $R^2$ fit statistic that we just discussed.  \n",
    "\n",
    "The **F-statistic** is the ratio of the variance explained by the model to the residuals. The **Prob(F-statistic)** is the probability of obtaining that value of the F-statistic (or greater) under the F-distribution implied by the degrees of freedom from the model which can be found in the Df Residuals and Df Model fields of the summary table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on individual predictors\n",
    "\n",
    "The second table is the table of tests on individual coefficients in the model. \n",
    "\n",
    "Each row in this table refers to one of the coefficients in the regression model. \n",
    "\n",
    "The first row is the intercept term (**Intercept**), and the later ones look at each of the predictors (**sleep_hours** and **work_hours** in this case). \n",
    "\n",
    "The columns give you all of the relevant information: \n",
    "\n",
    "- `coef`: is the actual estimate of $b$ (e.g., 125.96 for the intercept, and -8.9 for the  `sleep_hours` predictor).\n",
    "\n",
    "- `std err`: The second column is the standard error estimate $\\hat\\sigma_b$. \n",
    "\n",
    "- `t` and `P>|t|`: the is the $t$-statistic and $p$ value (P>|t|) for each of the tests of the estimated coefficient versus 0.\n",
    "\n",
    "- The final two columns show the 95% confidence intervals on the values of the parameters. \n",
    "\n",
    "The table doesn't list the degrees of freedom used in the $t$-test, which is always $N-K-1$ and is listed in the table above as **Df Residuals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating standardised regression coefficients\n",
    "\n",
    "One more thing that you might want to do is to calculate \"standardised\" regression coefficients, often denoted $\\beta$. \n",
    "\n",
    "There is more discussion of this issue in the standalone regression notebook.\n",
    "\n",
    "The rationale behind standardised coefficients goes like this: In a lot of situations, your variables are on fundamentally different scales and this makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. This is what **_standardised coefficients_** aim to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is quite simple: the standardised coefficients are the coefficients that you would have obtained if you'd converted all the variables to $z$-scores before running the regression.  \n",
    "\n",
    "By converting all the predictors to $z$-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a $\\beta$ value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of $\\beta$ than variable B, it is deemed to have a stronger relationship with the outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-scoring data\n",
    "\n",
    "First, we'll use the dataframe .apply() method. We give this function a an instruction or calcuation that we want to apply to each column of our data. In this case, z-scoring within column so that each column ends up with mean 0 and sd 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the zscore for each column we use the `stats.zscore()` function from the scipy.stats library we previously imported as stats (import scipy.stats as stats) and pass that to the apply() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df.apply() with input stats.zscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we re-run the regression the values of the estimated coefficients has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook introduces practical issues in running single and multi-variable linear regression in Python.\n",
    "\n",
    "There is substantial background material, additional explanations, and discussion of the assumptions of regression that can be found in the standalone regression notebook (3_regression_1.ipynb) and you are encouraged to work your way through that notebook to deepen your understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
